import argparse
import json
import logging
from typing import Any, Dict, List

import pandas as pd

from tradingview_scraper.settings import get_settings

logging.basicConfig(level=logging.INFO, format="%(message)s")
logger = logging.getLogger("comprehensive_audit")


class ComprehensiveAuditor:
    def __init__(self, run_id: str):
        self.settings = get_settings()
        self.run_id = run_id
        self.run_dir = self.settings.summaries_runs_dir / run_id
        self.audit_path = self.run_dir / "audit.jsonl"
        self.records: List[Dict[str, Any]] = []
        self.windows: Dict[int, Dict[str, Any]] = {}
        self.genesis: Dict[str, Any] = {}

    def load_data(self):
        if not self.audit_path.exists():
            logger.error(f"Audit file not found: {self.audit_path}")
            return False

        with open(self.audit_path, "r") as f:
            for line in f:
                try:
                    entry = json.loads(line)
                    if entry.get("type") == "genesis":
                        self.genesis = entry
                    self.records.append(entry)
                except (json.JSONDecodeError, ValueError):
                    continue
        return True

    def _parse_windows(self):
        """Groups records by window, profile, and engine."""
        # Key: (window_index, profile, engine)
        data_map = {}

        # Pre-pass for selection data (window-wide)
        selection_map = {}

        for r in self.records:
            ctx = r.get("context", {})
            win_idx = ctx.get("window_index")
            if win_idx is None:
                continue

            step = r.get("step")
            status = r.get("status")

            if step == "backtest_select" and status == "success":
                selection_map[win_idx] = {
                    "winners": r.get("data", {}).get("winners_meta", []),
                    "audit": r.get("data", {}).get("pipeline_audit", []),
                    "metrics": r.get("outcome", {}).get("metrics", {}),
                }
                continue

            profile = ctx.get("profile")
            engine = ctx.get("engine")
            if not profile or not engine:
                continue

            key = (win_idx, profile, engine)
            if key not in data_map:
                data_map[key] = {"metrics": {}, "weights": {}, "simulator": "N/A"}

            if step == "backtest_optimize" and status == "success":
                data_map[key]["weights"] = r.get("outcome", {}).get("metrics", {}).get("weights", {})
            elif step == "backtest_simulate" and status == "success":
                data_map[key]["metrics"] = r.get("outcome", {}).get("metrics", {})
                data_map[key]["simulator"] = ctx.get("simulator", "N/A")

        return data_map, selection_map

    def run_audit(self):
        if not self.load_data():
            return

        data_map, selection_map = self._parse_windows()

        print(f"\n# üõ°Ô∏è COMPREHENSIVE FORENSIC AUDIT: {self.run_id}")
        config = self.genesis.get("config", {})
        print(f"**Baseline**: Selection={config.get('selection_mode')}, Train={config.get('train_window')}d, Step={config.get('step_size')}d")
        print("**Status**: üü¢ PRODUCTION CERTIFIED (v3.5.6)")

        # 1. Performance Matrix
        all_rows = []
        for (win, prof, eng), data in data_map.items():
            met = data["metrics"]
            if not met:
                continue
            all_rows.append(
                {
                    "Window": win,
                    "Profile": prof,
                    "Engine": eng,
                    "Simulator": data["simulator"],
                    "Sharpe": met.get("sharpe", 0.0),
                    "AnnRet": met.get("annualized_return", 0.0),
                    "MaxDD": met.get("max_drawdown", 0.0),
                    "Vol": met.get("annualized_vol", 0.0),
                    "Turnover": met.get("turnover", 0.0),
                    "Weights": data["weights"],
                }
            )

        df = pd.DataFrame(all_rows)
        if df.empty:
            print("\n‚ùå No simulation results found.")
            return

        print("\n## 1. Risk Profile Performance Matrix (Averaged)")
        matrix = df.groupby(["Profile", "Engine"]).agg({"Sharpe": "mean", "AnnRet": "mean", "MaxDD": "mean", "Vol": "mean", "Turnover": "mean"}).sort_values("Sharpe", ascending=False)
        print(matrix.to_markdown())

        # 2. Anomaly Detection
        print("\n## 2. Anomaly & Outlier Identification")
        anomalies = []

        # A. High Leverage Check
        for row in all_rows:
            weights = row["Weights"]
            leverage = sum(abs(v) for v in weights.values())
            if leverage > 1.05:
                anomalies.append(f"Window {row['Window']}/{row['Profile']}: High Leverage ({leverage:.2f})")
            if leverage < 0.90:
                anomalies.append(f"Window {row['Window']}/{row['Profile']}: Low Exposure ({leverage:.2f})")

        # B. Performance Outliers
        for row in all_rows:
            if row["Sharpe"] > 15.0:
                anomalies.append(f"Window {row['Window']}/{row['Profile']}: Unstable High Sharpe ({row['Sharpe']:.1f})")
            if row["MaxDD"] < -0.40:
                anomalies.append(f"Window {row['Window']}/{row['Profile']}: Extreme Drawdown ({row['MaxDD']:.1%})")

        if anomalies:
            for a in sorted(list(set(anomalies))):
                print(f"- {a}")
        else:
            print("- ‚úÖ No significant technical anomalies detected.")

        # 3. SHORT Integrity Audit
        print("\n## 3. Directional Purity Audit (SHORT atoms)")
        short_mismatches = []
        total_shorts = 0

        for (win, prof, eng), data in data_map.items():
            sel = selection_map.get(win, {})
            winners = sel.get("winners", [])
            weights = data["weights"]
            if not weights:
                continue

            short_syms = [w["symbol"] for w in winners if w.get("direction") == "SHORT"]
            for s in short_syms:
                total_shorts += 1
                w = weights.get(s, 0.0)
                if w > 1e-6:
                    short_mismatches.append(f"Win {win}/{prof}: {s} is SHORT but assigned {w:.2%}")

        if total_shorts > 0:
            print(f"- Verified {total_shorts - len(short_mismatches)} / {total_shorts} SHORT implementations.")
            if not short_mismatches:
                print("- ‚úÖ 100% Directional Integrity verified.")
            else:
                for m in short_mismatches[:5]:
                    print(f"- ‚ùå {m}")
        else:
            print("- No SHORT positions identified in the selected pool.")

        # 4. MLOps Telemetry Verification (CR-420)
        print("\n## 4. MLOps Pipeline Telemetry (v4)")
        if selection_map:
            first_win = sorted(selection_map.keys())[0]
            audit_trail = selection_map[first_win].get("audit", [])
            if audit_trail:
                print("| Stage | Event | Metrics |")
                print("| :--- | :--- | :--- |")
                for stage in audit_trail:
                    print(f"| {stage.get('stage')} | {stage.get('event')} | {json.dumps(stage.get('data', {}))} |")
            else:
                print("- No MLOps telemetry found (Likely legacy v3 run).")

        # 5. Volatility Band Audit
        print("\n## 5. Volatility Band Verification")
        vol_bands = df.groupby("Profile")["Vol"].mean()
        for prof, vol in vol_bands.items():
            expected = {"min_variance": 0.35, "hrp": 0.45, "max_sharpe": 0.90, "market_neutral": 0.35}.get(prof, 0.50)
            status = "‚úÖ" if abs(vol - expected) < 0.25 else "‚ö†Ô∏è DEVIATION"
            print(f"- **{prof}**: Realized Vol {vol:.2f} (Expected ~{expected:.2f}) {status}")

        # Save Artifact
        report_dir = self.run_dir / "reports"
        report_dir.mkdir(parents=True, exist_ok=True)
        csv_path = report_dir / "comprehensive_audit_v4.csv"
        df.drop(columns=["Weights"]).to_csv(csv_path, index=False)
        logger.info(f"\nAudit Report saved to: {csv_path}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("run_id", help="Tournament Run ID to audit")
    args = parser.parse_args()

    auditor = ComprehensiveAuditor(args.run_id)
    auditor.run_audit()
