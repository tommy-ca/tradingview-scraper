# Design: Ray Native Filesystem Isolation

## 1. The Challenge
The current `scripts/parallel_orchestrator_native.py` achieves "Native" execution (importing Python classes instead of subprocesses) but bypasses Ray's filesystem isolation by executing `os.chdir(host_cwd)`.

While this works for local single-node execution, it violates "Cloud Native" principles:
1.  **Race Conditions**: Multiple workers writing to the same `host_cwd` (even if different subdirs) can cause lock contention (e.g., `__pycache__`, shared logs).
2.  **Cluster Incompatibility**: On a multi-node cluster, `host_cwd` from the driver node does not exist on worker nodes.
3.  **Dependency Leakage**: The worker relies on the host's `PYTHONPATH` and installed packages rather than a strictly defined `runtime_env`.

## 2. Ray Native Isolation (`runtime_env`)
The "True" Ray Native approach uses `runtime_env` to define the worker's context.

```python
ray.init(runtime_env={
    "working_dir": ".",      # Zips and uploads current dir to workers
    "excludes": ["artifacts/", "data/"], # Exclude heavy data to keep zip small
    "pip": ["./pyproject.toml"] # Install dependencies on the fly
})
```

When this is used:
- The worker starts in a temporary directory (e.g., `/tmp/ray/session_.../runtime_resources/...`).
- `os.getcwd()` returns this temporary path.
- The `Makefile` and code are present (unzipped).
- **CRITICAL ISSUE**: Artifacts generated by `make` (e.g., `export/run_id/*.json`) are written to this ephemeral directory and lost when the task ends.

## 3. Proposed Solution: The "Artifact Export" Pattern

To support true isolation while preserving data, we must decouple **Execution** from **Persistence**.

### 3.1. Execution (Isolated)
The worker runs the pipeline in its isolated `working_dir`. It generates data to `./export/run_id/` and `./data/run_id/`.

### 3.2. Persistence (Explicit)
Instead of relying on a shared filesystem, the worker must explicitly **export** its results.

#### Mechanism A: Return-by-Value (Small Data)
For metrics and logs, return them directly in the task result dictionary.

#### Mechanism B: Blob Storage / Shared Mount (Large Data)
The worker accepts a `storage_uri` (e.g., `s3://bucket/runs/` or `/mnt/nfs/runs/` or even `/abs/path/to/host/runs`).
At the end of execution, it performs a synchronized copy.

```python
# Pseudo-code for Worker
def run_pipeline(self, profile, run_id, output_uri):
    # 1. Run Locally (Isolated)
    pipeline.execute()
    
    # 2. Bundle Artifacts
    import shutil
    shutil.make_archive(f"run_{run_id}", 'zip', root_dir=".", base_dir=f"artifacts/summaries/runs/{run_id}")
    
    # 3. Export
    if output_uri.startswith("file://"):
        dest = output_uri.replace("file://", "")
        shutil.copy(f"run_{run_id}.zip", dest)
```

## 4. Implementation Steps (Phase 234)

### 4.1. Refactor `ProductionPipeline`
- Ensure it doesn't assume absolute paths that only exist on the host.
- Ensure it respects `TV_ARTIFACTS_DIR` env var (defaulting to `./artifacts`).

### 4.2. Update Orchestrator (`scripts/parallel_orchestrator_native.py`)
- Remove `os.chdir(host_cwd)`.
- Use `runtime_env` for proper setup.
- Implement an **Artifact Sync** step at the end of `run_pipeline`.
    - For local execution: Copy from isolated dir back to `host_cwd/artifacts`.

### 4.3. Validation
- Run `meta_ray_test`.
- Verify `working_dir` is NOT `host_cwd`.
- Verify artifacts appear in the host's `artifacts/` folder after completion.

## 5. Benefits
- **Zero-Shared-State**: Workers can run anywhere (cloud, different nodes).
- **Reproducibility**: Environment is strictly defined by `runtime_env`.
- **Scalability**: No file lock contention on the project root.
