# Audit Note: Windowing Effects on Temporal Fragility (Jan 2026)

This note compares two **production-parity** tournament smoke runs to answer:

> Does moving from `train/test/step = 120/20/20` to `180/40/20` reduce short-window Sharpe sign-flips and improve temporal stability?

It also captures why strict scoreboard candidates were empty in the larger-window probe prior to the antifragility-tail fix.

Follow-up (post-implementation validation):
- See `docs/specs/audit_smoke_regime_windowing_20260105_174600_vs_180000.md` for the corrected larger-window + realized-regime smoke (`20260105-180000`) and confirmed strict candidates becoming non-empty under `180/40/20`.

## Runs Compared

### Run A (Control): `20260105-170135`
- Windowing: `train/test/step = 120/20/20`
- Payload: `artifacts/summaries/runs/20260105-170135/grand_4d_tournament_results.json`
- Scoreboard: `artifacts/summaries/runs/20260105-170135/data/tournament_scoreboard.csv`
- Candidates: `artifacts/summaries/runs/20260105-170135/data/tournament_candidates.csv`

### Run B (Probe): `20260105-170324`
- Windowing: `train/test/step = 180/40/20` (overlapping tests; `test_window > step_size`)
- Payload: `artifacts/summaries/runs/20260105-170324/grand_4d_tournament_results.json`
- Scoreboard: `artifacts/summaries/runs/20260105-170324/data/tournament_scoreboard.csv`
- Candidates: `artifacts/summaries/runs/20260105-170324/data/tournament_candidates.csv` (empty pre-fix)

## Coverage + Structural Differences (Important)

The stability probe differs from control in **two** meaningful ways:

1. **Longer test window**: 20d → 40d (reduces single-regime noise)
2. **Longer train window**: 120d → 180d (shifts the *start* of the first test forward)

Observed walk-forward coverage:
- Run A: 11 windows spanning **2025-02-19 → 2026-01-02**
- Run B: 7 windows spanning **2025-05-15 → 2026-01-02**

Implication: improvements in sign-flips / fragility are partly attributable to the later start date (Run B does not include the early 2025 window that was strongly negative in Run A).

## Temporal Fragility + Sharpe Sign-Flips (Observed)

Below are the core profiles that drove the original concern (`temporal_fragility` dominated by short-window sign flips):
- Baseline: `market/market`
- Baseline diagnostic: `market/raw_pool_ew`
- Candidate-like: `custom/hrp`

### Run A (120/20/20): High fragility + multiple sign flips

Key findings:
- `market/raw_pool_ew` is the primary outlier: **fragility ~6–9** and **flip rate ~0.40**.
- Several profiles show **large negative Sharpe** in the first 20d window (`2025-02-19 → 2025-03-18`), followed by strongly positive windows → classic sign-flip driver.

Examples (per-simulator, computed from window Sharpe series in tournament payload):
- `market/market`: fragility ~**1.76–1.85**, sign flips **2 / 10 transitions**
- `custom/hrp`: fragility ~**2.02–2.08**, sign flips **1–2 / 10 transitions**
- `market/raw_pool_ew`: fragility ~**6.12–9.53**, sign flips **4 / 10 transitions**, 4 negative windows

Extreme-window driver (Run A):
- Many profiles have **minimum Sharpe** at `2025-02-19 → 2025-03-18`:
  - `market/market`: min Sharpe ≈ **-4.47** (custom/nautilus), **-5.38** (cvx)
  - `market/raw_pool_ew`: min Sharpe ≈ **-4.84** (custom/nautilus), **-5.81** (cvx)
  - `market/benchmark`: min Sharpe ≈ **-5.71** (custom/nautilus), **-6.65** (cvx)
  - `custom/hrp`: min Sharpe ≈ **-6.02** (custom/nautilus), **-7.00** (cvx)

This single-window polarity shift is a large portion of the measured `temporal_fragility` in the 20d setup.

### Run B (180/40/20): Substantially reduced fragility; flips collapse for most key profiles

Key findings:
- `market/market` and `custom/hrp` show **zero sign flips** in this run.
- `market/raw_pool_ew` improves dramatically (fragility falls from ~6–9 → ~0.96), but **still has sign flips** (2 / 6 transitions) and one negative window.

Examples:
- `market/market`: fragility ~**0.53–0.56**, sign flips **0 / 6 transitions**
- `custom/hrp`: fragility ~**0.43–0.48**, sign flips **0 / 6 transitions**
- `market/raw_pool_ew`: fragility ~**0.96**, sign flips **2 / 6 transitions**, 1 negative window

Interpretation:
- Increasing the train/test windows makes the Sharpe series far less polarity-sensitive, and `raw_pool_ew` becomes a usable reference baseline rather than an extreme outlier.
- This supports treating `raw_pool_ew` fragility as a **calibration signal** (rather than an absolute gate), especially for short-window setups.

## Strict Scoreboard Outcomes (Gating)

### Run A candidates (non-empty)
- Strict candidates exist (baseline rows pass): `artifacts/summaries/runs/20260105-170135/data/tournament_candidates.csv`
- Non-baseline profiles are mostly blocked by other gates (e.g., simulator parity).

### Run B candidates (empty, pre-fix) — root cause

All rows failed with `missing:af_dist` in `artifacts/summaries/runs/20260105-170324/data/tournament_scoreboard.csv`.

Root cause:
- `antifragility_dist` uses `q=0.95` tails.
- With overlapping-window stitching, the effective unique daily observations were about **160** in this probe run.
- Expected tail size at `q=0.95` is roughly `(1-q) * n_obs ≈ 8`.
- The previous `min_tail=10` requirement made `antifragility_dist.is_sufficient=False` for every config, so `af_dist` was not emitted and strict gating treated it as missing.

Fix (already implemented):
- `tradingview_scraper/utils/metrics.py` now makes tail sufficiency coherent with sample size and records `min_tail_required`.
- Rerun the `180/40/20` probe to validate strict candidates under the updated antifragility distribution logic.

## Eligibility Stabilizer Sanity

Both runs show **no** `BONKUSDT` occurrences in optimize weights when grepping:
- `rg -n \"BONKUSDT\" artifacts/summaries/runs/20260105-170135/audit.jsonl`
- `rg -n \"BONKUSDT\" artifacts/summaries/runs/20260105-170324/audit.jsonl`

## Next Action (Recommended)

To fully close the loop on “larger windows fix instability + strict candidates remain non-empty”, rerun the stability probe after the antifragility-tail fix:

```bash
TV_RUN_ID=<NEW_ID> BACKTEST_TRAIN=180 BACKTEST_TEST=40 BACKTEST_STEP=20 make port-test
make tournament-scoreboard-run RUN_ID=<NEW_ID>
```

Acceptance:
- `tournament_candidates.csv` non-empty (at minimum baseline `market/market` passes).
- `af_dist` is populated (no global `missing:af_dist` failure).

## Issue Collected: Window `regime` Label Semantics (Train vs Test)

### Symptom
- In Run A (`20260105-170135`), the first test window `2025-02-19→2025-03-18` is labeled `regime=QUIET` while the realized Sharpe is strongly negative (e.g. `market/market` Sharpe ≈ `-4.47`).

### Root Cause (Confirmed)
- The tournament assigns `window["regime"]` using the **training slice**, not the realized test slice.
- In `scripts/backtest_engine.py`, the regime is computed as:
  - `regime = self.detector.detect_regime(train_data_raw)[0]`
- That `regime` value is then attached to the **test window record**:
  - `results[...]["windows"].append({"regime": regime, ... "start_date": test_start, "end_date": test_end, ...})`

Interpretation:
- The stored `regime` is a **decision-time (pre-test) market state estimate** (computed at `train_end == test_start`),
  not a realized “this test window was QUIET/TURBULENT” label.

### Why This Looks Like a Mismatch (Even When Detector Is Working)
- `MarketRegimeDetector`’s primary regime label (`QUIET|NORMAL|TURBULENT|CRISIS`) is driven mainly by **volatility ratio / turbulence / clustering**, not directional return.
- Therefore, “QUIET + negative Sharpe” can be valid (quiet does not imply positive returns), but the larger confusion here is that the label is **not realized-on-test**.

### Proposed Remediation (Spec-Level)
- Rename / disambiguate regime fields in window payloads:
  - `decision_regime` (computed from `train_data_raw`, used to parameterize optimization)
  - `realized_regime` (computed from `test_data` or a stable benchmark proxy, used for regime robustness summaries)
- Persist the supporting diagnostics alongside labels (at least):
  - `decision_regime_score`
  - `decision_quadrant` (from `detect_quadrant_regime`)
- Update “worst-regime” scoring logic to use the realized regime label (otherwise `worst_regime` is not a realized regime result).

Spec reference:
- `docs/specs/regime_labeling_semantics_v1.md`

## Validation: Regime Semantics v1 (Post-Implementation)

### Smoke A (Decision Regime Fields)
- Run: `20260105-174600` (train/test/step `120/20/20`)
- Payload: `artifacts/summaries/runs/20260105-174600/data/tournament_results.json`
  - Window records include:
    - `decision_regime`, `decision_regime_score`, `decision_quadrant`
    - `realized_regime` present but `null` (default; `TV_ENABLE_REALIZED_REGIME` not set)
- Scoreboard: `artifacts/summaries/runs/20260105-174600/data/tournament_scoreboard.csv`
  - Worst-regime summaries match decision regime grouping.

### Smoke B (Realized Regime Enabled)
- Run: `20260105-180000` (train/test/step `180/40/20`)
- Env: `TV_ENABLE_REALIZED_REGIME=1`
- Payload: `artifacts/summaries/runs/20260105-180000/data/tournament_results.json`
  - Window records include:
    - `realized_regime`, `realized_regime_score`, `realized_quadrant`
- Scoreboard: `artifacts/summaries/runs/20260105-180000/data/tournament_scoreboard.csv`
  - Worst-regime summaries prefer realized regime when present (verified by recomputing worst regime from payload windows and matching scoreboard output).

Note:
- Run `20260105-175200` also validates realized-regime fields and scoreboard preference, but it retained default windowing (`120/20/20`) due to pre-fix Makefile override propagation.
