# DataOps Architecture v1: The Decoupled Lakehouse

## 1. MLOps/DataOps Principles
To scale the platform from a "Script Runner" to an "Institutional Platform", we adopt the following DataOps principles:

### 1.1 Separation of Concerns
- **Data Engineering (Data Cycle)**: Responsible for availability, freshness, and quality of the raw asset universe.
- **Quantitative Research (Alpha Cycle)**: Responsible for selecting assets and optimizing weights.
- **Contract**: The Data Cycle produces a certified "Golden Copy" in the Lakehouse. The Alpha Cycle consumes this copy as Read-Only input.

### 1.2 Reproducibility via Snapshotting
- **The Problem**: A mutable Lakehouse breaks backtest reproducibility.
- **The Solution**: Every Alpha Run (Production Pipeline) begins by **snapshotting** the necessary subset of the Lakehouse into its immutable `run_dir`.
- **Constraint**: No external API calls allowed inside the Alpha Cycle.

### 1.3 Quality Gates as a Service
- Toxic Data Filtering (e.g., ADA +157,000% spike) moves from "Reaction" (in backtest) to "Prevention" (in ingestion).
- The Lakehouse must never contain known-toxic data.

## 2. Architecture Diagram

```mermaid
graph TD
    subgraph "Data Cycle (flow-data)"
        A[Universe/Scanner Configs] -->|Run Scanners| B[Discovery Engine]
        B -->|Generate| C[Candidate List (JSON)]
        C -->|Input| D[Ingestion Service]
        D -->|Check Freshness| E{Lakehouse State}
        E -->|Fresh| F[No-Op (Idempotent)]
        E -->|Stale/Missing| G[Fetch & Clean]
        G -->|Toxic Filter| H[Data Lakehouse (Parquet)]
        H -->|Update| I[Lakehouse Catalog (JSON)]
        H -->|Validation| Gap[Gap Repair Service]
    end

    subgraph "Alpha Cycle (flow-alpha)"
        I -->|Read| J[Data Selector]
        H -->|Read| J
        J -->|Snapshot| K[Run Artifacts (Immutable)]
        K --> L[Selection Engine]
        L --> M[Synthesis Engine]
        M --> N[Optimization Engine]
    end
```

## 3. Technical Specifications

### 3.1 The Lakehouse Contract
- **Registry**: `docs/design/lakehouse_schema_registry.md`
- **Location**: `data/lakehouse/`
- **Format**: `EXCHANGE_SYMBOL_1d.parquet` (Snappy compression).
- **Schema**: `timestamp (int64), open, high, low, close, volume`.
- **Index**: `catalog.json` containing metadata (freshness, start_date, end_date, data_quality_score).

### 3.2 Ingestion Service (`scripts/services/ingest_data.py`)
A unified CLI tool replacing ad-hoc `data-fetch` calls.
- **Input**: A list of `candidate_*.json` files generated by Scanners.
- **Idempotency Logic**:
    - For each symbol in candidates:
        1.  Check `data/lakehouse/{symbol}_1d.parquet`.
        2.  If file exists AND `mtime < 12 hours` (or configured freshness), **SKIP**.
        3.  Else, **QUEUE** for fetch.
- **Operations**:
    1.  **Batch Fetch**: Execute fetching for the queued subset.
    2.  **Sanitize**: Apply `TOXIC_THRESHOLD` (>500% returns) before writing.
    3.  **Validate**: Check for gaps > 5 days.
    4.  **Commit**: Atomic write to Parquet.

### 3.3 Data Selector (`scripts/prepare_portfolio_data.py` Refactor)
- **Mode**: `source="lakehouse_only"` (Default for Production).
- **Operation**:
    1.  Read `candidates.json` (from upstream Discovery in Data Cycle or passed explicit file).
    2.  Check Lakehouse for these assets.
    3.  **Copy** Parquet files from Lakehouse to `artifacts/runs/<ID>/data/`.
    4.  **Fail Fast**: If any required asset is missing/stale in Lakehouse, **ABORT** the pipeline. Do NOT attempt network calls.

### 3.4 Feature Store
- **Service**: `scripts/services/ingest_features.py`
- **Contract**: Produces `data/lakehouse/features/tv_technicals_1d.parquet`.
- **Trigger**: Runs daily after Data/Meta ingestion.

### 3.5 Gap Repair Service (`scripts/services/repair_data.py` - Migrated)
- **Role**: Specialized service for detecting and healing gaps in Lakehouse data.
- **Logic**: Iterates through active symbols in `symbols.parquet`, detects date gaps > `threshold`, and triggers targeted re-fetches.
- **Integration**: Runs as part of `flow-data` (Weekly or On-Demand), not every run.

## 4. Operational Workflow

### Daily Routine (Cron/Airflow)
1.  **Step 1: Data Cycle** (`make flow-data`)
    - **Discovery**: Generates `export/<date>/candidates.json`.
    - **Ingestion**: Updates Lakehouse from candidates.
    - **Meta/Features**: Enriches Lakehouse.
    - **Output**: Validated Lakehouse snapshot.

2.  **Step 2: Alpha Cycle** (`make flow-production`)
    - **Trigger**: Runs *after* Data Cycle completion.
    - **Input**: `export/<date>/candidates.json` (passed via `CANDIDATES_FILE` or resolved via RUN_ID).
    - **Operation**:
        - **Aggregation**: Builds `returns_matrix.parquet` from Lakehouse.
        - **Selection/Optimization**: Pure computation.
    - **Constraint**: **NO** network I/O permitted.

### Maintenance Routine
1.  **Gap Repair**: `make data-repair` (Runs `repair_data.py`).
2.  **Audit**: `make data-audit` (Checks for new toxic data or drifts).
